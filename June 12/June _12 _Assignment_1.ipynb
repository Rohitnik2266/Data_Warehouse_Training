{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2191995-2155-4cbc-a8a8-095d0e98f602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Intialize the SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c35e9b1a-0fd0-4054-a4ce-a0310c9642f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3525126442407722#setting/sparkui/0611-043339-3vb7b9iv/driver-2404666304674668943\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7217a97dd4d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"SalesDataProcessing\")\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab49af53-936c-4fcc-b642-3ed28f5f03e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Create Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b949ce-9d83-4e05-87c0-be7a8f8c4f0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- OrderID: long (nullable = true)\n |-- Customer: string (nullable = true)\n |-- Items: array (nullable = true)\n |    |-- element: map (containsNull = true)\n |    |    |-- key: string\n |    |    |-- value: string (valueContainsNull = true)\n |-- Region: string (nullable = true)\n |-- Amount: double (nullable = true)\n\n+-------+--------+--------------------------------------------------------------+------+------+\n|OrderID|Customer|Items                                                         |Region|Amount|\n+-------+--------+--------------------------------------------------------------+------+------+\n|101    |Ali     |[{Product -> Laptop, Qty -> 1}, {Product -> Mouse, Qty -> 2}] |Asia  |1200.0|\n|102    |Zara    |[{Product -> Tablet, Qty -> 1}]                               |Europe|650.0 |\n|103    |Mohan   |[{Product -> Phone, Qty -> 2}, {Product -> Charger, Qty -> 1}]|Asia  |890.0 |\n|104    |Sara    |[{Product -> Desk, Qty -> 1}]                                 |US    |450.0 |\n+-------+--------+--------------------------------------------------------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "data = [\n",
    "    Row(OrderID=101, Customer=\"Ali\", Items=[{\"Product\":\"Laptop\", \"Qty\":1}, {\"Product\":\"Mouse\", \"Qty\":2}], Region=\"Asia\", Amount=1200.0),\n",
    "    Row(OrderID=102, Customer=\"Zara\", Items=[{\"Product\":\"Tablet\", \"Qty\":1}], Region=\"Europe\", Amount=650.0),\n",
    "    Row(OrderID=103, Customer=\"Mohan\", Items=[{\"Product\":\"Phone\", \"Qty\":2}, {\"Product\":\"Charger\", \"Qty\":1}], Region=\"Asia\", Amount=890.0),\n",
    "    Row(OrderID=104, Customer=\"Sara\", Items=[{\"Product\":\"Desk\", \"Qty\":1}], Region=\"US\", Amount=450.0)\n",
    "]\n",
    "df_sales= spark.createDataFrame(data)\n",
    "df_sales.printSchema()\n",
    "df_sales.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40ee7a4a-903b-4f9b-b492-d24ddb4d804d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Working with JSON & Nested Fields**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e882309-6e47-4fb6-b498-78e0aca47a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after flattening:\n+-------+--------+------+------+-------+---+\n|OrderID|Customer|Region|Amount|Product|Qty|\n+-------+--------+------+------+-------+---+\n|101    |Ali     |Asia  |1200.0|Laptop |1.0|\n|101    |Ali     |Asia  |1200.0|Mouse  |2.0|\n|102    |Zara    |Europe|650.0 |Tablet |1.0|\n|103    |Mohan   |Asia  |890.0 |Phone  |2.0|\n|103    |Mohan   |Asia  |890.0 |Charger|1.0|\n|104    |Sara    |US    |450.0 |Desk   |1.0|\n+-------+--------+------+------+-------+---+\n\nroot\n |-- OrderID: long (nullable = true)\n |-- Customer: string (nullable = true)\n |-- Region: string (nullable = true)\n |-- Amount: double (nullable = true)\n |-- Product: string (nullable = true)\n |-- Qty: double (nullable = true)\n\nTotal quantity sold per product:\n+-------+--------+\n|Product|TotalQty|\n+-------+--------+\n| Laptop|     1.0|\n|  Mouse|     2.0|\n| Tablet|     1.0|\n|  Phone|     2.0|\n|Charger|     1.0|\n|   Desk|     1.0|\n+-------+--------+\n\nNumber of orders per region:\n+------+----------+\n|Region|OrderCount|\n+------+----------+\n|  Asia|         2|\n|Europe|         1|\n|    US|         1|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#1.Flatten Items using explode()\n",
    "from pyspark.sql.functions import explode, col  \n",
    "df_exploded = df_sales.withColumn(\"Item\", explode(\"Items\")) \\\n",
    "                      .withColumn(\"Product\", col(\"Item.Product\")) \\\n",
    "                      .withColumn(\"Qty\", col(\"Item.Qty\").cast(\"double\")) \\\n",
    "                      .drop(\"Items\", \"Item\")\n",
    "print(\"Schema after flattening:\")\n",
    "df_exploded.show(truncate=False)\n",
    "df_exploded.printSchema()\n",
    "#2.Count total quantity sold per product\n",
    "from pyspark.sql.functions import sum\n",
    "print(\"Total quantity sold per product:\")\n",
    "df_exploded.groupBy(\"Product\").sum(\"Qty\").withColumnRenamed(\"sum(Qty)\", \"TotalQty\").show()\n",
    "#3.Count number of orders per region\n",
    "print(\"Number of orders per region:\")\n",
    "df_sales.groupBy(\"Region\").count().withColumnRenamed(\"count\", \"OrderCount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf4ff1f-83ce-44e1-9da4-1f18b903ce7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Using when and otherwise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7fd220c-c09c-44d0-b600-28b282a28d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HighValueOrder column:\n+-------+------+--------------+\n|OrderID|Amount|HighValueOrder|\n+-------+------+--------------+\n|    101|1200.0|           Yes|\n|    102| 650.0|            No|\n|    103| 890.0|            No|\n|    104| 450.0|            No|\n+-------+------+--------------+\n\nShippingZone column:\n+------+------------+\n|Region|ShippingZone|\n+------+------------+\n|  Asia|      Zone A|\n|Europe|      Zone B|\n|    US|      Zone C|\n+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#4.Create a new column HighValueOrder\n",
    "from pyspark.sql.functions import when\n",
    "print(\"HighValueOrder column:\")\n",
    "df_sales = df_sales.withColumn(\"HighValueOrder\", when(col(\"Amount\") > 1000, \"Yes\").otherwise(\"No\"))\n",
    "df_sales.select(\"OrderID\", \"Amount\", \"HighValueOrder\").show()\n",
    "#5.Add a column ShippingZone\n",
    "df_sales = df_sales.withColumn(\"ShippingZone\",\n",
    "    when(col(\"Region\") == \"Asia\", \"Zone A\")\n",
    "    .when(col(\"Region\") == \"Europe\", \"Zone B\")\n",
    "    .when(col(\"Region\") == \"US\", \"Zone C\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "print(\"ShippingZone column:\")\n",
    "df_sales.select(\"Region\", \"ShippingZone\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350ed47d-77a6-4085-bc12-bcc4e05403f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Temporary & Permanent Views**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8afb8d3-9785-43fb-a8a9-b2b9b77096a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders by region:\n+------+----------+\n|Region|OrderCount|\n+------+----------+\n|  Asia|         2|\n|Europe|         1|\n|    US|         1|\n+------+----------+\n\nAverage amount per region:\n+------+---------+\n|Region|AvgAmount|\n+------+---------+\n|  Asia|   1045.0|\n|Europe|    650.0|\n|    US|    450.0|\n+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#6.Register df_sales as a temporary view named \n",
    "df_sales.createOrReplaceTempView(\"sales_view\")\n",
    "#7.SQL Queries\n",
    "#Count orders by region\n",
    "print(\"Orders by region:\")\n",
    "spark.sql(\"SELECT Region, COUNT(*) as OrderCount FROM sales_view GROUP BY Region\").show()\n",
    "#Average amount per region\n",
    "print(\"Average amount per region:\")\n",
    "spark.sql(\"SELECT Region, AVG(Amount) as AvgAmount FROM sales_view GROUP BY Region\").show()\n",
    "#8.Create a permanent view using saveAsTable() \n",
    "df_sales.write.mode(\"overwrite\").saveAsTable(\"sales_permanent_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afb08d5d-2814-44fa-9d2c-cb7015ad2e35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL Queries via Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3744f6a1-173b-40f2-9ed4-9864013701c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders with more than 1 item:\n+-------+--------+--------------------+------+------+--------------+------------+\n|OrderID|Customer|               Items|Region|Amount|HighValueOrder|ShippingZone|\n+-------+--------+--------------------+------+------+--------------+------------+\n|    101|     Ali|[{Product -> Lapt...|  Asia|1200.0|           Yes|      Zone A|\n|    103|   Mohan|[{Product -> Phon...|  Asia| 890.0|            No|      Zone A|\n+-------+--------+--------------------+------+------+--------------+------------+\n\nCustomers with Amount > 800:\n+--------+------+\n|Customer|Amount|\n+--------+------+\n|     Ali|1200.0|\n|   Mohan| 890.0|\n+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#9.Filter orders with more than 1 item\n",
    "print(\"Orders with more than 1 item:\")\n",
    "spark.sql(\"SELECT * FROM sales_view WHERE size(Items) > 1\").show()\n",
    "#10.Customers with Amount > 800\n",
    "print(\"Customers with Amount > 800:\")\n",
    "spark.sql(\"SELECT Customer, Amount FROM sales_view WHERE Amount > 800\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfd99500-00bd-46ff-8561-701e5279adfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Saving as Parquet and Reading Again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24052e8b-0e03-4b68-88b1-5dfa4a5c5105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#11.Save exploded DataFrame as Parquet partitioned by Region\n",
    "df_exploded.write.mode(\"overwrite\").partitionBy(\"Region\").parquet(\"/mnt/parquet/sales_by_region\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3366ec-700a-438b-9f8a-946d3a17811b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n|Product|TotalQty|\n+-------+--------+\n|  Phone|     2.0|\n|Charger|     1.0|\n| Laptop|     1.0|\n|  Mouse|     2.0|\n| Tablet|     1.0|\n|   Desk|     1.0|\n+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#12.Read Parquet & Group by Product\n",
    "parquet = spark.read.parquet(\"/mnt/parquet/sales_by_region\")\n",
    "parquet.groupBy(\"Product\").sum(\"Qty\").withColumnRenamed(\"sum(Qty)\", \"TotalQty\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88c70f2e-4860-450d-8287-dd3a85f5214a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "june12(set1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}